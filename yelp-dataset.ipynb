{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKh6Bu45F6GG"
   },
   "source": [
    "### Outline\n",
    "\n",
    "#### Restaurant Recommendations for (User, City)\n",
    "\n",
    "##### Steps\n",
    "\n",
    "1. Encode Restaurants as an Embedding that captures two things:\n",
    "  - Restaurants that have similar review text should be closer\n",
    "    - Tried here: Mean of Glove word embeddings after text clean up.\n",
    "    - TF-IDF Weighted average of embeddings might improve results\n",
    "    - Different ways of text encoding should be explored (BERT et al.)\n",
    "  - Restaurants that are rated similarly by different users should be closer (irrespective of whether the actural stars rating is the same)\n",
    "    - Tried here: Use Word2Vec architecture\n",
    "      - User1, 4 Stars: (Res1, Res2, Res3)\n",
    "      - User2, 1 Stars: (Res1, Res4, Res3)\n",
    "      - => Res2 and Res4 are similar in some way because they appear in the company of similar restaurants (Res1, Res3). User1 loves the group, User2 hates the group, but there is something common about the group. Word2Vec tries to capture this. \n",
    "\n",
    "\n",
    "2. Encode Users as an Embedding that captures their preferences based on their preferred and not preferred restaurants.\n",
    "  - Tried here:\n",
    "    - User Embedding = Preferred Restaurants Embedding - Not Preferred Restaurants Embedding\n",
    "    - Preferred Restaurants Embedding: Avarage of Positively Reviewed (Stars > 3) Restaurants\n",
    "    - Not Preferred Restaurants Embedding: Average of Negatively Reviewed (Stars < 3) Restaurants). Neutral reviews ignored.\n",
    "\n",
    "\n",
    "3. Train a Neural Network Model to take in a User Embedding and a Restaurant Embedding and predict the Stars Rating\n",
    " - Criterion: Mean Squared Error\n",
    " - MSE Loss observed on Test Data: 0.01 (That looks quite good; wondering if I made any mistakes. Need more scruitiny!)\n",
    " - Alternative: This could be a model that takes one-hot vectors of Users and Restaurants and have an Embedding layer in between that can be learned. Might not scale with huge number of Restaurants and Users.\n",
    "\n",
    "\n",
    "4. Use the above model to predict Stars Rating of all Restaurants for a given (User, City), sort descending and recommend top 5.\n",
    "  - Sample function given at the end of the notebook\n",
    "  - Alternative: To avoid the Neural Network Model, recommendations may be given as sorting Restaurants ascending by Cosine Similarity between User Embedding and Restaurant Embeddings for the given City. It could be a simpler model to serve if it works decently (Right now gives very different results compared to Neural Net, and haven't looked at how good the results are). \n",
    "\n",
    "\n",
    "#### Price Range Prediction for Restaurants (Not Done)\n",
    "\n",
    "##### Steps\n",
    "\n",
    "1. Train a Neural Network Model to take in a Restaurant Embedding based on only reviews text and predict Price Range\n",
    "  - Glove word embeddings based method mentioned above can be used as a start\n",
    "\n",
    "\n",
    "2. Use the above model for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io5SzV7IkjIF"
   },
   "source": [
    "### Set Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27157,
     "status": "ok",
     "timestamp": 1620065060855,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "Ar2IBskMElkd",
    "outputId": "ee703552-96a9-4eb0-b6f6-aaa6a6afbed2"
   },
   "outputs": [],
   "source": [
    "env = 'local' # 'colab'\n",
    "\n",
    "if env == 'colab':\n",
    "    from google.colab import drive\n",
    "    \n",
    "    drive.mount('/content/gdrive')\n",
    "    drivepath = 'gdrive/My Drive'\n",
    "\n",
    "else:\n",
    "    drivepath = '/Users/cijogeorge/Google Drive'\n",
    "\n",
    "datapath = drivepath + '/github/yelp-dataset'\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiX9OY2LEg68"
   },
   "source": [
    "### Import Libraries and Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 110740,
     "status": "ok",
     "timestamp": 1620065144446,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "NuphW6jREg7C",
    "outputId": "064ad91f-6dfa-4999-d5c4-c85bcd79ffb5"
   },
   "outputs": [],
   "source": [
    "!pip install pandas==1.2.4\n",
    "!pip install tqdm==4.60.0\n",
    "!pip install gensim==4.0.1\n",
    "!pip install python-Levenshtein==0.12.2\n",
    "!pip install nltk==3.6.2\n",
    "!pip install torch==1.8.1\n",
    "\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "import multiprocessing\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn import random_projection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "                    level=logging.INFO)\n",
    "\n",
    "# Load Glove Word Embeddings for Encoding Reviews Text\n",
    "glove_word_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmvuV85LEg7E"
   },
   "source": [
    "### Load data\n",
    "\n",
    "- Note: J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194528,
     "status": "ok",
     "timestamp": 1620065261563,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "plIyILdPEg7E",
    "outputId": "45a0c43b-b252-408b-e1d6-d8eeee148837"
   },
   "outputs": [],
   "source": [
    "df_businesses_file = datapath + '/data/df_businesses.pkl'\n",
    "df_reviews_file = datapath + '/data/df_reviews.pkl'\n",
    "\n",
    "# df_businesses: \n",
    "#       Pandas dataframe with restaurants from yelp business dataset\n",
    "# df_reviews: \n",
    "#       Pandas dataframe with restaurant reviews from yelp reviews dataset\n",
    "\n",
    "# Load df_businesses data from disk if they exist, else generate\n",
    "if os.path.isfile(df_businesses_file):\n",
    "    print('Loading file:', df_businesses_file)\n",
    "\n",
    "    with open(df_businesses_file, 'rb') as f:\n",
    "        df_businesses = pickle.load(f)\n",
    "    \n",
    "else:\n",
    "    print('Generating file:', df_businesses_file)\n",
    "\n",
    "    # Import the data (chunksize returns jsonReader for iteration)\n",
    "    businesses = pd.read_json(\"./data/yelp_academic_dataset_business.json\", \n",
    "                              lines=True, orient='columns', chunksize=100000)\n",
    "\n",
    "    # Read the data\n",
    "    frames = []\n",
    "    for business in tqdm(businesses):\n",
    "        business = \\\n",
    "          business[business['categories'].str.contains('Restaurant.*') == True].reset_index()\n",
    "        frames.append(business)\n",
    "\n",
    "    df_businesses = pd.concat(frames, sort=False)\n",
    "    del frames\n",
    "    del businesses\n",
    "\n",
    "    df_businesses = df_businesses[df_businesses['is_open'] == 1]\n",
    "    df_businesses.set_index('business_id', inplace=True, drop=True)\n",
    "        \n",
    "    with open(df_businesses_file, 'wb') as f:\n",
    "        pickle.dump(df_businesses, f)\n",
    "    \n",
    "# Load df_reviews data from disk if they exist, else generate\n",
    "if os.path.isfile(df_reviews_file):\n",
    "    print('Loading file:', df_reviews_file)\n",
    "\n",
    "    with open(df_reviews_file, 'rb') as f:\n",
    "        df_reviews = pickle.load(f)\n",
    "else:\n",
    "    print('Generating file:', df_reviews_file)\n",
    "\n",
    "    reviews = pd.read_json(\"./data/yelp_academic_dataset_review.json\", \n",
    "                           lines=True, orient='columns', chunksize=100000)\n",
    "    frames = []\n",
    "    for review in tqdm(reviews):\n",
    "        review = review[review['business_id'].isin(df_businesses.index)]\n",
    "        frames.append(review)\n",
    "\n",
    "    df_reviews = pd.concat(frames, sort=False)\n",
    "\n",
    "    del frames\n",
    "    del reviews\n",
    "    \n",
    "    with open(df_reviews_file, 'wb') as f:\n",
    "        pickle.dump(df_reviews, f)\n",
    "\n",
    "# Select indices to keep aside test data for model evaluation \n",
    "train_indices, test_indices = train_test_split(range(0, len(df_reviews)), \n",
    "                                               test_size=0.20, \n",
    "                                               random_state=42)\n",
    "\n",
    "# business_reviews: \n",
    "#       Dictionary mapping Restaurants and their review texts\n",
    "# user_restaurants: \n",
    "#       Dictionary with mapping between Users and their preferred (stars >3) \n",
    "#       and not preferred (stars < 3) Restaurants.\n",
    "\n",
    "business_reviews_file = datapath + '/data/business_reviews.pkl'\n",
    "user_restaurants_file = datapath + '/data/user_restaurants.pkl'\n",
    "\n",
    "# Load business_reviews and user_restaurants from file if exists, else generate\n",
    "if os.path.isfile(business_reviews_file) and os.path.isfile(user_restaurants_file):\n",
    "    print('Loading files:')\n",
    "    print(business_reviews_file)\n",
    "    print(user_restaurants_file)\n",
    "    \n",
    "    with open(business_reviews_file, 'rb') as f:\n",
    "        business_reviews = pickle.load(f)\n",
    "\n",
    "    with open(user_restaurants_file, 'rb') as f:          \n",
    "        user_restaurants = pickle.load(f)\n",
    "    \n",
    "else:\n",
    "    print('Generating files:')\n",
    "    print(business_reviews_file)\n",
    "    print(user_restaurants_file)\n",
    "    \n",
    "    business_reviews = {}\n",
    "    user_restaurants = {}\n",
    "\n",
    "    # Only train indices are used to fetch data. Not touching test data.\n",
    "    for index in tqdm(train_indices):\n",
    "        business_reviews.setdefault(\n",
    "            df_reviews.iloc[index]['business_id'], []).append(\n",
    "                df_reviews.iloc[index]['text'])\n",
    "\n",
    "        if df_reviews.iloc[index]['stars'] > 3:\n",
    "            user_restaurants.setdefault(\n",
    "                df_reviews.iloc[index]['user_id'], {}).setdefault(\n",
    "                    'positive', []).append(df_reviews.iloc[index]['business_id'])\n",
    "\n",
    "        if df_reviews.iloc[index]['stars'] < 3:\n",
    "            user_restaurants.setdefault(\n",
    "                df_reviews.iloc[index]['user_id'], {}).setdefault(\n",
    "                    'negative', []).append(df_reviews.iloc[index]['business_id'])  \n",
    "\n",
    "    with open(business_reviews_file, 'wb') as f:\n",
    "        pickle.dump(business_reviews, f)\n",
    "\n",
    "    with open(user_restaurants_file, 'wb') as f:          \n",
    "        pickle.dump(user_restaurants, f)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77eLeGVwEg7F"
   },
   "source": [
    "### Generate Restaurant User Stars Model using Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2979,
     "status": "ok",
     "timestamp": 1620065264565,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "Z8-34w92Eg7F",
    "outputId": "447cb783-7b2c-43ca-fb91-85ed68b775c4"
   },
   "outputs": [],
   "source": [
    "restaurants_w2v_model_file = datapath + '/data/restaurants_w2v_model.pkl'\n",
    "\n",
    "# Load the model if exists, else generate\n",
    "if os.path.isfile(restaurants_w2v_model_file):\n",
    "    print('Loading model:', restaurants_w2v_model_file)\n",
    "    \n",
    "    with open(restaurants_w2v_model_file, 'rb') as f:\n",
    "        restaurants_w2v_model = pickle.load(f)\n",
    "    \n",
    "else:\n",
    "    print('Generating model:', restaurants_w2v_model_file)\n",
    "    \n",
    "    # Group restaurants with same stars given by a user\n",
    "    restaurant_groups = {}\n",
    "\n",
    "    reviews = {'user_id': list(df_reviews['user_id']),\n",
    "               'business_id': list(df_reviews['business_id']),\n",
    "               'stars': list(df_reviews['stars'])}\n",
    "\n",
    "    # Only train indices are used to fetch data. Not touching test data.\n",
    "    for i in tqdm(train_indices):\n",
    "        user_id = reviews['user_id'][i]\n",
    "        business_id = reviews['business_id'][i]\n",
    "        stars = reviews['stars'][i]\n",
    "\n",
    "        restaurant_groups.setdefault(user_id, {}).setdefault(\n",
    "            stars, []).append(business_id)\n",
    "\n",
    "    # Generate restaurant lists for Word2Vec training.\n",
    "    #     Idea: Restaurants that appear in similar \"context\" are similar.\n",
    "    #           Context: Obtained same stars by a user.\n",
    "    same_stars_restaurants = []\n",
    "    for user_id in tqdm(restaurant_groups):\n",
    "        for stars in restaurant_groups[user_id]:\n",
    "            same_stars_restaurants.append(restaurant_groups[user_id][stars])\n",
    "\n",
    "    # Word2Vec Model Training\n",
    "    restaurants_w2v_model = Word2Vec(sentences=same_stars_restaurants, \n",
    "                            vector_size=50, \n",
    "                            window=1000, \n",
    "                            min_count=1, \n",
    "                            workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "\n",
    "    with open(restaurants_w2v_model_file, 'wb') as f:\n",
    "        pickle.dump(restaurants_w2v_model, f)\n",
    "\n",
    "    del same_stars_restaurants\n",
    "    del reviews \n",
    "    del restaurant_groups\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWY4nJ6xEg7G"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2970,
     "status": "ok",
     "timestamp": 1620065264566,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "am-uDKuMEg7H",
    "outputId": "c8a304cf-11f8-4034-c100-a0f16bd0c911"
   },
   "outputs": [],
   "source": [
    "# Utils to clean text\n",
    "\n",
    "def _remove_url(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def _remove_non_alphabets(text):\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "\n",
    "def _lowercase(text):\n",
    "    return str(text).lower()\n",
    "\n",
    "def _tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def _remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def _lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(word=word, pos='v') for word in tokens]\n",
    "\n",
    "def _remove_shortwords(tokens, length=2):\n",
    "    return [word for word in tokens if len(word) > length]\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    text = _remove_url(text)\n",
    "    text = _remove_non_alphabets(text)\n",
    "    text = _lowercase(text)\n",
    "    \n",
    "    tokens = _tokenize(text)\n",
    "    tokens = _remove_stopwords(tokens)\n",
    "    tokens = _lemmatize(tokens)\n",
    "    tokens = _remove_shortwords(tokens, length=2)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Utils to Generate/ Fetch Restaurant and User Embeddings\n",
    "\n",
    "def get_restaurant_embedding(business_id):\n",
    "    return business_embeddings.get(business_id, [])\n",
    "\n",
    "def get_user_embedding(user_id):\n",
    "    preferred_restaurants = user_restaurants.get(user_id, {}).get('positive', [])\n",
    "    not_preferred_restaurants = user_restaurants.get(user_id, {}).get('negative', [])\n",
    "    \n",
    "    embedding = []\n",
    "    \n",
    "    if len(preferred_restaurants):\n",
    "        preferred_embedding = \\\n",
    "            np.mean([get_restaurant_embedding(business_id) \n",
    "                     for business_id in preferred_restaurants], axis=0)\n",
    "        embedding = preferred_embedding\n",
    "        \n",
    "    if len(not_preferred_restaurants):\n",
    "        not_preferred_embedding = \\\n",
    "            np.mean([get_restaurant_embedding(business_id) \n",
    "                     for business_id in not_preferred_restaurants], axis=0)\n",
    "    \n",
    "        if len(embedding):\n",
    "            embedding -= not_preferred_embedding\n",
    "                    \n",
    "    return list(embedding)\n",
    "\n",
    "def generate_restaurant_embedding(business_id, userstars=True, reviews=True):\n",
    "    _embedding = []\n",
    "    \n",
    "    if userstars:\n",
    "        _embedding.extend(_generate_restaurant_userstars_embedding(business_id))\n",
    "    \n",
    "    if reviews: \n",
    "        _embedding.extend(_generate_restaurant_reviews_embedding(business_id))\n",
    "        \n",
    "    return _embedding\n",
    "\n",
    "def _generate_restaurant_reviews_embedding(business_id):    \n",
    "    reviews_tokens = []\n",
    "    for text in business_reviews[business_id]:\n",
    "        tokens = clean_tokenize(text)\n",
    "        reviews_tokens.extend(tokens)\n",
    "        \n",
    "    reviews_tokens = [word for word in reviews_tokens \n",
    "                      if word in glove_word_model]\n",
    "    _embedding = list(np.mean([glove_word_model[word] \n",
    "                               for word in reviews_tokens], axis=0))\n",
    "    \n",
    "    return _embedding\n",
    "\n",
    "def _generate_restaurant_userstars_embedding(business_id):\n",
    "    _embedding = list(restaurants_w2v_model.wv[business_id])\n",
    "        \n",
    "    return _embedding\n",
    "\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwuMoV6rEg7I"
   },
   "source": [
    "### Generate Restaurant Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6361,
     "status": "ok",
     "timestamp": 1620065267962,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "QQGDQQEPEg7I",
    "outputId": "acd2604a-b7d4-41aa-d989-d6629e8e6418"
   },
   "outputs": [],
   "source": [
    "# business_embeddings: \n",
    "#`      Dictionary mapping Restaurants and their generated embeddings\n",
    "\n",
    "# Load Restaurant Embeddings from file if exits, else generate\n",
    "business_embeddings_file = datapath + '/data/business_embeddings.pkl'\n",
    "\n",
    "if os.path.isfile(business_embeddings_file):\n",
    "    print('Loading file:', business_embeddings_file)\n",
    "    \n",
    "    with open(business_embeddings_file, 'rb') as f:\n",
    "        business_embeddings = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    print('Generating file:', business_embeddings_file)\n",
    "    \n",
    "    business_embeddings = {}\n",
    "    \n",
    "    for business_id in tqdm(business_reviews):\n",
    "        business_embeddings[business_id] = generate_restaurant_embedding(business_id)\n",
    "        \n",
    "    with open(business_embeddings_file, 'wb') as f:\n",
    "        pickle.dump(business_embeddings, f)\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6355,
     "status": "ok",
     "timestamp": 1620065267962,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "a-pObHBlEg7J",
    "outputId": "23d5a922-8d51-49b9-949a-fe52c50ff053"
   },
   "outputs": [],
   "source": [
    "len(get_restaurant_embedding('AWsOwlorVHRSpgPJy1I0eg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6350,
     "status": "ok",
     "timestamp": 1620065267963,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "AG_YDJyPEg7J",
    "outputId": "634f56b5-15b5-4e36-f85d-52919577912a"
   },
   "outputs": [],
   "source": [
    "print(get_user_liked_restaurants('q_QQ5kBBwlCcbL1s4NVK3g', 'Atlanta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1266,
     "status": "ok",
     "timestamp": 1620070052688,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "vabVXi6eEg7J",
    "outputId": "ac8d2eae-fa53-46f0-fb6c-2fb8da8a031b"
   },
   "outputs": [],
   "source": [
    "print(get_recommendations('q_QQ5kBBwlCcbL1s4NVK3g', 'Atlanta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6892,
     "status": "ok",
     "timestamp": 1620065268515,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "iQNuUfK0C3E3",
    "outputId": "eb3abc42-6b4b-45b8-9f18-bbf50184349e"
   },
   "outputs": [],
   "source": [
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZpW-7KxjVT2"
   },
   "source": [
    "### Simple Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6lWnb8KEg7K",
    "outputId": "a2cd4744-3666-4642-e9fc-9f00a1cef633"
   },
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, indices):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.indices = indices\n",
    "        self.fallback_data = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            features, target = self._get_data(idx)\n",
    "            return (features, target)\n",
    "            \n",
    "        except:\n",
    "            return self._get_fallback_data()\n",
    "\n",
    "    def _get_fallback_data(self):\n",
    "        if self.fallback_data is not None:\n",
    "            return (self.fallback_data[0], self.fallback_data[1])\n",
    "        \n",
    "        for idx in range(0, len(self.indices)):            \n",
    "            try:\n",
    "                features, target = self._get_data(idx)\n",
    "                \n",
    "                if self.fallback_data is None:\n",
    "                    self.fallback_data = (features, target)\n",
    "                \n",
    "                return (features, target)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        \n",
    "    def _get_data(self, idx):\n",
    "        user_embedding = get_user_embedding(df_reviews.iloc[self.indices[idx]]['user_id'])\n",
    "        restaurant_embedding = get_restaurant_embedding(df_reviews.iloc[self.indices[idx]]['business_id'])\n",
    "\n",
    "        assert len(user_embedding) == 100\n",
    "        assert len(restaurant_embedding) == 100\n",
    "\n",
    "        features = np.array(user_embedding + restaurant_embedding, dtype=float)\n",
    "        target = np.array(df_reviews.iloc[self.indices[idx]]['stars'] - 1, dtype=float)\n",
    "\n",
    "        assert 0 <= target <= 4\n",
    "        \n",
    "        return (torch.Tensor(features), torch.Tensor(target))\n",
    "       \n",
    "\n",
    "# Neural Network Architecture\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(200, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.final = nn.Linear(50, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.dropout(\n",
    "            self.relu(\n",
    "                self.hidden(x)))\n",
    "        final = self.relu(self.final(hidden))\n",
    "        \n",
    "        return final\n",
    "\n",
    "\n",
    "# Set device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "\n",
    "# Load the Neural Network Model if exits, else train\n",
    "nn_model_file = datapath + '/data/nn_model.pt'\n",
    "\n",
    "if os.path.isfile(nn_model_file):\n",
    "    print('Loading Neural Network Model from file: ', nn_model_file)\n",
    "    nn_model = NeuralNetwork().to(device)\n",
    "    nn_model.load_state_dict(torch.load(nn_model_file))\n",
    "    nn_model.eval()\n",
    "\n",
    "else:\n",
    "    print('Training Neural Network Model')\n",
    "\n",
    "    # Train \n",
    "    \n",
    "    # Epoch is set to 1 due to time & resource constraints\n",
    "    num_epochs = 1\n",
    "    batch_size = 128\n",
    "\n",
    "    train = ReviewsDataset(train_indices)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Send model to device\n",
    "    nn_model = NeuralNetwork().to(device)\n",
    "\n",
    "    # Define loss & optimizer\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    nn_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (features, targets) in enumerate(train_loader):\n",
    "            targets = targets.view(targets.size(0), 1)\n",
    "\n",
    "            # Move tensors to the configured device\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = nn_model(features)\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Save the model         \n",
    "    print('Saving model to file: ', nn_model_file)\n",
    "    torch.save(nn_model.state_dict(), nn_model_file)\n",
    "    print('Done.')\n",
    "\n",
    "\n",
    "# Test the model using tbe test data kept aside\n",
    "print('Running on test data')\n",
    "\n",
    "nn_model.eval()\n",
    "\n",
    "test = ReviewsDataset(test_indices)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    \n",
    "    for features, targets in tqdm(test_loader):\n",
    "        targets = targets.view(targets.size(0), 1)\n",
    "\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs = nn_model(features)\n",
    "        loss += loss_function(outputs, targets)\n",
    "        total += targets.size(0)\n",
    "        \n",
    "    print('Avg. loss on test data: {} %'.format(loss/total))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVEX-4oHulZt"
   },
   "source": [
    "### Get recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1096,
     "status": "ok",
     "timestamp": 1620072982414,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "9RzudMHiuhqH",
    "outputId": "e2d029c7-0af9-489d-aa43-759bea44ff4f"
   },
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "\n",
    "# Utils for recommendations\n",
    "\n",
    "nn_model = NeuralNetwork().to(device)\n",
    "nn_model.load_state_dict(torch.load(nn_model_file))\n",
    "nn_model.eval()\n",
    "\n",
    "# Use method='embedding_similarity' to use cosine similarity between\n",
    "# User Embedding and Restaurant Embedding for scoring\n",
    "def get_recommendations(user_id, city, method='neural_network'):\n",
    "    df_businesses_subset = df_businesses[df_businesses['city'] == city]\n",
    "        \n",
    "    user_embedding = get_user_embedding(user_id)\n",
    "    \n",
    "    scores = []\n",
    "    for business_id in df_businesses_subset.index:\n",
    "        try:\n",
    "            restaurant_embedding = get_restaurant_embedding(business_id)\n",
    "            if method == 'embedding_similarity':\n",
    "                scores.append(cosine_similarity(restaurant_embedding, \n",
    "                                                user_embedding))\n",
    "            else:\n",
    "                features = np.array(user_embedding + restaurant_embedding, \n",
    "                                  dtype=float)\n",
    "                features = torch.Tensor(features)\n",
    "                features = features.to(device)\n",
    "        \n",
    "                output = nn_model(features)\n",
    "                scores.append(float(output))\n",
    "\n",
    "        except:\n",
    "            if method == 'embedding_similarity':\n",
    "                scores.append(0.5)\n",
    "\n",
    "            else:\n",
    "                scores.append(3)\n",
    "        \n",
    "    df_businesses_subset['reco_score'] = scores\n",
    "    df_businesses_subset_sorted = df_businesses_subset.sort_values(by='reco_score', \n",
    "                                                                   ascending=False)\n",
    "    \n",
    "    return df_businesses_subset_sorted\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample recommendation function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1866,
     "status": "ok",
     "timestamp": 1620072986229,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "rHhz6nxG1QSx",
    "outputId": "abd699cc-04e3-4d7e-f5b2-904b97efea74"
   },
   "outputs": [],
   "source": [
    "# Using Neural Network Model\n",
    "\n",
    "reco = get_recommendations('q_QQ5kBBwlCcbL1s4NVK3g', 'Atlanta', method='neural_network')\n",
    "print(reco[['name', 'address', 'reco_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1354,
     "status": "ok",
     "timestamp": 1620072990809,
     "user": {
      "displayName": "Cijo George",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZQmWQ5dOc_YgS0TxO8d5eQqmaju5sknaa_U9TFww=s64",
      "userId": "13168617627188463796"
     },
     "user_tz": -330
    },
    "id": "zltyyEdP12ry",
    "outputId": "53edf28d-4e25-4a54-b5e6-92f7b02ba5e0"
   },
   "outputs": [],
   "source": [
    "# Using Embedding Similarity (not evaluated)\n",
    "\n",
    "reco = get_recommendations('q_QQ5kBBwlCcbL1s4NVK3g', 'Atlanta', method='embedding_similarity')\n",
    "print(reco[['name', 'address', 'reco_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "yelp-dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
